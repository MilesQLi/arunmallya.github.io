
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
  <head>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Layout and HTML pieces the work of Jon Barron http://www.cs.berkeley.edu/~barron/ */
      a {
      color: #1772d1;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09227;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
      }
    </style>

    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <title>Backprop</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

  </head>

  <body>
    <table width="850" border="0" align="center" cellpadding="20">
      <tr>
	      <td> 

          <p align="center"><font size="6">Backpropagation</font></p>  
          <hr>

          <p align="center"> 
            <b> <a name="multiple_layers">Multiple Layers</a></b></br></br> 
            Consider a network made of multiple layers, connected to some loss function \( E(\cdot) \) at the top.</br> 
            We'll look at a part of this network connected as shown in the figure below. We'll index these layers
            by \(\{\cdots, L-2, L-1, L, \cdots\}\). Further, we'll index individual neurons in these layers by \(i, j\) 
            as shown in the figure.</br></br>
            <img width="30%" src="images/layers.png" alt="Current Input">
          </p>
          <hr>

          <p align="center"> 
            <b> <a name="single_layer">A Single Layer</a></b></br></br> 
            Let's zoom into a single layer, \(L\), made up of a number of neurons, each of which receives
            outputs from the neurons of layer \(L-1\) as input. The outputs of neurons of layer \(L\) are denoted by
            \(o^L_i\), and those of layer \(L-1\) are denoted by \(o_j^{L-1}\). </br>
            (Remember that we decided to index neurons in layers \(L\) and \(L-1\) by \(i\) and \(j\) resp.)</br>
            The weight connecting neuron \(i\) in layer \(L-1\) to neuron \(j\) in layer \(L\) is denoted by \(w_{i,j}\).</br> 
            </br></br>
            <img width="60%" src="images/layer.png" alt="Current Input">
          </p>
          <hr>

          <p align="center"> 
            <b> <a name="eqs">Neuron Equations</a></b></br></br> 
            The output of a neuron in layer \(L\) is given by:
            $$ o_i^L = \varphi(\text{net}_i^L) = \varphi\left(\sum_{k=1}^n w_{i,k}\cdot o_k^{L-1}\right)$$
            where \(\varphi(\cdot)\) is some non-linearity such as the sigmoid or tanh.</br></br>

            Using the chain rule, we'll break up the derivative of the network error \(E\) with respect to the neuron weight \(w_{i,j}\):
            \begin{align}
              \Delta w_{i,j} = \frac{\partial E}{\partial w_{i,j}} &= \left( \frac{\partial E}{\partial \text{net}_i^L} \right)
              \cdot \left( \frac{\partial \text{net}_i^L}{\partial w_{i,j}} \right) = \delta_i^L \cdot \left( \frac{\partial \text{net}_i^L}{\partial w_{i,j}} \right)\\
            \end{align}

            Now, we'll solve each component above one by one:
            <ol>
              <li> 
                \(
                  \left(\displaystyle\frac{\partial \text{net}_i^L}{\partial w_{i,j}}\right) = \displaystyle\frac{\partial \left(\displaystyle\sum_{k=1}^n w_{i,k}\cdot o_k^{L-1}\right)}{\partial w_{i,j}} = o_j^{L-1}
                \)
              </li></br></br>

              <li> 
                \( \delta_i^L \) is provided by the layer \(L+1\) above.</br>
                This means that at layer \(L\), we'll have to compute and pass \( \delta_j^{L-1} \) to the layer \(L-1\) below.</br> Let's compute it then!
                </br></br>
                \begin{align}
                  \delta_j^{L-1} = \frac{\partial E}{\partial \text{net}_j^{L-1}} &= 
                  \left( \frac{\partial E}{\partial o_j^{L-1}} \right)\cdot\left( \frac{\partial o_j^{L-1}}{\partial \text{net}_j^{L-1}} \right) \\
                  &= 
                  \left( \displaystyle\sum_{i=1}^m \frac{\partial E}{\partial \text{net}_i^L} \cdot \frac{\partial \text{net}_i^L}{\partial o_j^{L-1}} \right)\cdot\left( \frac{\partial o_j^{L-1}}{\partial \text{net}_j^{L-1}} \right)
                \end{align}

                <p align="center"> 
                  The sum above arises because \(o_j^{L-1}\) is used by all neurons \(\{i\}\) in layer \(L\).</br> 
                  Thus, the total error flowing downwards into \(\{j\}\) is the sum of the errors from each of the neurons in layer \(L\).</br></br>
                  Let's examine the components:
                </p>

                <ol style="display:table; margin:0 auto;">
                  <li type="a">
                    \(
                      \displaystyle\frac{\partial E}{\partial \text{net}_i^L} = \delta_i^L
                    \)</br> 
                    by definition and is passed from the layer \(L+1\) above layer \(L\)
                  </li></br>
                  <li type="a">
                    \(
                      \displaystyle\frac{\partial \text{net}_i^L}{\partial o_j^{L-1}} = \displaystyle\frac{\partial\left(\displaystyle\sum_{k=1}^n w_{i,k}\cdot o_k^{L-1}\right)}{\partial o_j^{L-1}} = w_{i, j}
                    \) 
                  </li></br>
                  <li type="a">
                    \(
                      \displaystyle\frac{\partial o_j^{L-1}}{\partial \text{net}_j^{L-1}} = \displaystyle\frac{\partial \left(\varphi(\text{net}_j^{L-1})\right)}{\partial \text{net}_j^{L-1}} = \varphi'(\text{net}_j^{L-1})
                    \)</br>
                    If \(\varphi(\cdot)\) is a sigmoid, then \(\varphi'(\cdot) = \varphi(\cdot) (1 - \varphi(\cdot))\)</br>
                    If \(\varphi(\cdot)\) is tanh, then \(\varphi'(\cdot) =  (1 - \varphi^2(\cdot))\)
                  </li>
                </ol>
              </li>
            </ol>

            $$
            \boxed{
              \therefore \delta_j^{L-1} = \varphi'(net_j^{L-1}) \cdot \sum_{i=1}^m \delta_i^L \cdot w_{i,j}}
            $$

          </p>
          <hr>

          <p align="center"> 
            <b> <a name="mat_eqs">Matrix Form</a></b></br></br>
            Putting the above equations into matrix form, we get:
            $$
              \boxed{\left( \Delta W \right)_{m\times n} = {\left(\delta^L\right)}_{m\times 1} \times {\left( (o^{L-1})^T \right)}_{1\times n}}
            $$ 
            and
            $$
              \boxed{\left( \delta^{L-1} \right)_{n\times 1} = \left(\varphi'(net^{L-1})\right)_{n\times 1} \circ \left(W^T_{n\times m} \times \delta^L_{m\times 1} \right)}
            $$  
          </p>
          <hr>

          </br></br></br></br></br></br></br></br>

        </td>
      </tr>
    </table>
  </body>
</html>
